@phdthesis{Hinz2016Evolutionary,
  address = {Hamburg, Germany},
  type = {MSc},
  title = {An {{Evolutionary Approach}} to {{Optimize}} the {{Hyperparameters}} of {{Convolutional Neural Networks}}},
  abstract = {Many modern learning algorithms, such as artificial neural networks, require the practitioner to manually set the values of many hyperparameters before the learning process can begin. However, with modern deep neural networks the evaluation of a given hyperparameter setting takes a lot of time and the search space for the optimization procedure is usually very high dimensional. We propose the usage of a genetic algorithm (GA) to optimize the hyperparameters specifically for convolutional neural networks (CNNs). Additionally, we suggest to use a lower dimensional representation of the original data to quickly identify promising areas in the hyperparameter space. We compare the results of the GA and the hyperparameter optimization on lower dimensional inputs to the ``Tree of Parzen Estimators'' (TPE) algorithm, an optimization algorithm based on Bayesian probabilities.

Our experiments show that the GA using lower dimensional data representations for earlier generations needs less time to find hyperparameters that perform similar or better than those found by the TPE algorithm. Preliminary results also indicate that under some conditions the GA may be better suited to handle external constraints such as memory limitations. Our results suggest that using genetic algorithms and low dimensional data representations for the earlier generations during the optimization process are a promising way to find appropriate hyperparameters for CNNs and a given problem.},
  school = {Universit{\"a}t Hamburg},
  author = {Hinz, Tobias and Wermter, Stefan and Navarro-Guerrero, Nicol{\'a}s and Magg, Sven},
  month = nov,
  year = {2016}
}
