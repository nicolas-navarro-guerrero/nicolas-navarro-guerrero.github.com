@mastersthesis{Stahlhut2014Learning,
  abstract = {Giving interactive feedback, other than well/bad done alone, allows to learn faster in Reinforcement Learning. If we reduce the number of wrong actions the learner takes, e.g. by undoing bad actions, we can reduce the number of steps necessary to learn a task. However, the amount of help needed is not thoroughly researched. This thesis aims to answer this question with two tasks. In both tasks the learner learns to move one arm each to reach for an arbitrary position within its reach. The first task is with a shoulder as joint alone, the second with a shoulder and an elbow. We will discover that the advantage of undoing bad actions manifests itself only in the arm with elbow and shoulder and not in the arm with a shoulder as only joint. We will see that not only did the interactive learner learn faster, it learned to take less steps to reach its target, too. Furthermore, it needed less configuration effort to get good result, while the non-interactive learner did not learn very well in some configurations. If both learners learn well, we need a lot of feedback to show an increase in the behaviour, as well as the learning speed. If the difference between both learners is large, even a probability of 10\% of interactive feedback shows an improvement of both properties. With a probability of 60\% or more, both properties become more stable than the property of the original learner without interactive feedback.},
  address = {{Hamburg, Germany}},
  language = {eng},
  title = {{Learning to Reach with Interactive Reinforcement Learning}},
  type = {MSc},
  school = {{Universit\"{a}t Hamburg}},
  author = {Stahlhut, Chris},
  collaborator = {Wermter, Stefan and Weber, Cornelius and Navarro-Guerrero, Nicol\'{a}s},
  month = oct,
  year = {2014},
}
