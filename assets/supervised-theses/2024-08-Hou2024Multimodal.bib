@phdthesis{Hou2024Multimodal,
  type = {{{MSc}}},
  title = {Multimodal {{Object Recognition}}},
  author = {Hou, Qizhen},
  year = {2024},
  month = aug,
  address = {Hanover, Germany},
  abstract = {In the field of robotics, using multiple senses for recognizing objects performs better than relying on just one sense. This study investigates how visual and haptic signals can be combined for recognizing objects and suggests two different approaches. It uses various Vision Transformer (ViT) models to extract features from visual and haptic inputs. The combination of these multimodal features is achieved by using different methods such as simple concatenation and weighted summation. The experimental results show that ViT models have great potential for integrating visual and haptic information. Among the different ViT models evaluated, the Data-efficient Image Transformer (DeiT) performs well in both visual and haptic single-mode recognition tasks. On the other hand, the Shifted Window (Swin Transformer) excels in multimodal object recognition tasks. When combining features from different modalities, direct concatenation proves to be the most effective method, as it retains all the information without adding complexity to the model during training.},
  school = {Gottfried Wilhelm Leibniz Universit{\"a}t Hannover}
}
