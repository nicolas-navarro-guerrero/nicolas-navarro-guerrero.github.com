---
tags: [thesis, bachelor, uhh, wtm, computer science, reinforcement learning, appetitive aversive stimuli]
categories:

thumbnail:
file: 
bibtex: "2015-04-Sperling2015Structural.bib"
slides:
library:
supplementary:
code:

year: 2015
month: 03

supervisors: [
      {
        family: Weber,
        given: Cornelius
      },
    ]

examiners: [
      {
        family: Weber,
        given: Cornelius
      },
      {
        family: Navarro-Guerrero,
        given: Nicolás
      },
    ]

citation: {
		"id": 5450,
		"type": "thesis",
		"title": "Structural Interference in Approach and Avoidance Behaviors by a Neurally Controlled Robot",
		"publisher": "Universität Hamburg",
		"publisher-place": "Hamburg, Germany",
		"number-of-pages": "49",
		"genre": "BSc",
		"event-place": "Hamburg, Germany",
		"abstract": "Humans learn to adopt to high complex, high dimensional real world problems. The separation of neural circuits into modules, that act on disjunct functions enables the human to adopt.\n\nDopamine in the basal ganglia encodes differences of expected and experienced reward. External stimuli do not only occur in positive matter, but also appear as negative signals. The amygdala represents strong negative external stimuli as fear. The nuclei complex adopts fast to fear experiences. Over time, discrimination takes place and the fear signals are understood more precisely. There are two types of dopamine releases that reinforce learning. Phasic mid brain dopamine fires on positive external stimuli. Tonic dopmaine is suppressed on negative external stimuli.\nThe phasic activation, from positive stimuli, is unproportional higher than the suppression of tonic dopamine. As a result a \"dopamine-ramp\" occurs. The ramp disadvantages learning on negative stimuli in the basal ganglia. The abstraction of dopamine as temporal differential (TD) error is used to teach an agent to navigate towards a state with reward. As well as for reward in navigation, artificial neural networks can be used to map input signal to states with fear. The artificial neural network maps input signals to a Q-values. Q-Values encode volitional behavior as an answer of the input signals.\n\nContrary external stimuli, reinforced on one neural structure, cause interference. An artificial multi layer perceptron (MLP) shows no behavior adaptation to solutions that are reinforced with contrary signals. An agent, controlled by a single structure MLP, is not capable to learn the mapping of input signals to actions. The positive TD error for reward contradicts the fear reinforcement. As a result of the interference, the agent does not learn the non-linear approximation of the input-ouput mapping. Two neural circuits can have distinct weight changes via TD error for pleasant reward and TD error, that resembles fear. Each neural circuit of the separated MLP has half the amount of neurons in the hidden layer. The separation into two neural circuits, show disjunct stepwise optimization towards fear and reward learning. The dissolved structure shows adaptation to conjugate external stimuli. In practice, the discretization of neural structures into disjunct functions, is not proportional to the complexity of the minimizing error function. Contrary action selections of the agent extend exploration.",
		"bibtex": "Sperling2015Structural",
		"language": "en",
		"author": [
			{
				"family": "Sperling",
				"given": "Jonas"
			}
		],
		"issued": {
			"year": "2015",
			"month": "03",
		}
	}
---
